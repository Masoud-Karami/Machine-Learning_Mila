# -*- coding: utf-8 -*-
"""#tp3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A2U08A3kvZV_w5NaIxH1IkXT3b7vAf0l

**# Masoud Karami**      **2050716**


[INF8953CE(Fall 2020) : Machine Learning](http://sarathchandar.in/teaching/ml/fall2020/)
[Assignment #3](https://drive.google.com/file/d/1UNZ-ozmZ2OsG9L7XTzKBEud9A6aZaOgN/view?usp=sharing) Due on : Nov 16, 10:00 pm

* You have to submit the pdf copy of the report on gradescope before the deadline. If you
handwrite your solutions, you need to scan the pages, merge them to a single pdf file and
submit. Mark page 1 for outline items $Late\ Submission$ and $Verbosity$.

* When asked to report the parameters, save the corresponding parameters in a text file
(.txt) with the following template for the name: 
`Assignment1 <Matricule> <section> <sub-question > <sub-sub-question>`. Submit all the text files and codes compressed
to a single file `<your-matricule>.zip` on Moodle.

$Note:$ gradescope doesnâ€™t accept `.zip`.

* Be precise with your explanations in the report. Unnecessary verbosity will be penalized.
* You are free to use libraries with general utilities, such as `matplotlib`, `numpy` and `scipy` for
`python`. You can use pre-existing implementation of the algorithms available in scikit-learn
package. Do not use NLTK or any other NLP libraries for pre-processing.!

---
---

# Sentiment Classification

In this assignment, we will design a sentiment classifier for classifying the sentiment of the
reviews. This is a *Natural Language Processing (NLP)* task where the input is a natural
language text and output is the sentiment label. We will consider two different review
datasets: Yelp reviews for restaurants and IMDB reviews for movies.

# Yelp dataset

The Yelp dataset consists of 7000 reviews in the training set, 1000 reviews in the validation
set, and 2000 reviews in the test set. This is a 5 class problem where each review is classified
into one of the five ratings with rating-5 being the best score and rating-1 being the worst
score.

# IMDB dataset

IMDB dataset consists of 15000 reviews in training set, 10000 reviews in validation set, and
25000 reviews in test set. This is a 2 class problem with class 1 being positive sentiment and
class 0 being negative sentiment.
"""

# import sys
# import os


# filelist = [ f for f in os.listdir('/content/') if f.endswith(".txt") or f.endswith(".csv")]
# for f in filelist:
#     os.remove(os.path.join('/content/', f))
# sys.modules[__name__].__dict__.clear()

"""> # **1.** 
Most of the algorithms described in the class expects input as a vector. However, the
reviews are natural language text of varying number of words. So the first step would
be to convert this varying length movie review to a fixed length vector representation.
We will consider two different ways of vectorizing the natural language text: binary
bag-of-words representation and frequency bag-of-words representation (as explained in
the end of the assignment). Convert both the datasets into both these representations
and turn in the converted datasets. Instruction for dataset format is given in the end of
the assignment (do not include the dataset in the report).
"""

# print('Question #1', '\n')


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import string
import operator
from sklearn import preprocessing
from sklearn.naive_bayes import BernoulliNB
from sklearn import metrics
from sklearn import tree
from sklearn.svm import LinearSVC


data_path = '/content/'
Yelp_train = pd.read_csv(data_path + "yelp-train.txt", sep='\t', header=None)
Yelp_test = pd.read_csv(data_path + "yelp-test.txt", sep='\t', header=None)
Yelp_valid = pd.read_csv(data_path + "yelp-valid.txt", sep='\t', header=None)
IMDB_train = pd.read_csv(data_path + "IMDB-train.txt", sep='\t', header=None)
IMDB_test = pd.read_csv(data_path + "IMDB-test.txt", sep='\t', header=None)
IMDB_valid = pd.read_csv(data_path + "IMDB-valid.txt", sep='\t', header=None)


#  removal of punctuation
out = set(string.punctuation)
seperator = '#'*100

def bin_bag_of_words(data_set):
  counti = data_set[0].count()
  for counter in range(counti):
    senti = data_set[0][counter]
    senti = ''.join(char for char in senti if char not in out)
    senti = senti.lower()
    data_set[0][counter] = senti
  return data_set

Yelp_train = bin_bag_of_words(Yelp_train)

# Commented out IPython magic to ensure Python compatibility.
# # binary bag of words
# %%time
# 
# Yelp_train = bin_bag_of_words(Yelp_train)
# Yelp_test = bin_bag_of_words(Yelp_test)
# Yelp_valid = bin_bag_of_words(Yelp_valid)
# IMDB_train = bin_bag_of_words(IMDB_train)
# IMDB_test = bin_bag_of_words(IMDB_test)
# IMDB_valid = bin_bag_of_words(IMDB_valid)

Yelp_train = Yelp_train.values
Yelp_test = Yelp_test.values
Yelp_valid = Yelp_valid.values

IMDB_train = IMDB_train.values
IMDB_test = IMDB_test.values
IMDB_valid = IMDB_valid.values
# print(Yelp_train)
# print(IMDB_train)

def top_myriad(data_set):
  pre_data = {}
  for col in range(data_set.shape[0]):
    sep = data_set[col][0].split()
    for idems in range(len(sep)):
      if sep[idems] in pre_data:
        pre_data[sep[idems]] += 1
      else:
        pre_data[sep[idems]] = 1
  return pre_data

top_train_Yelp = []
top_train_IMDB = []
sortit_Yelp = top_myriad(Yelp_train).items()
sortit_IMDB = top_myriad(IMDB_train).items()
sort_train_Yelp = sorted(sortit_Yelp, key=operator.itemgetter(1), reverse=True)
sort_train_IMDB = sorted(sortit_IMDB, key=operator.itemgetter(1), reverse=True)


# only consider the training set
for i in range(10000):
  top_train_Yelp.append(sort_train_Yelp[i][0])
  top_train_IMDB.append(sort_train_IMDB[i][0])

def vector_bin_bag(data_set, n, Yelp = True):
  vector = np.zeros(shape=(data_set.shape[0], n), dtype=int)
  top_train = top_train_Yelp if Yelp else top_train_IMDB
  for col in range(data_set.shape[0]):
    mot = data_set[col][0].split(' ')
    for counter in range(n):
      if top_train[counter] in mot:
        vector[col][counter] = 1
      else:
        vector[col][counter] = 0
  return vector

# Commented out IPython magic to ensure Python compatibility.
# # takes time, be patient please
# %%time
# 
# 
# Yelp_train_vec = vector_bin_bag(Yelp_train, 10000, True)
# Yelp_test_vec = vector_bin_bag(Yelp_test, 10000, True)
# Yelp_valid_vec = vector_bin_bag(Yelp_valid, 10000, True)

class_Yelp_train = Yelp_train[:,1].astype(str)
class_Yelp_test = Yelp_test[:,1].astype(str)
class_Yelp_valid = Yelp_valid[:,1].astype(str)

# count the frequency
# sort based on frequency
# descending order
# pick top 10,000 words, ignore the rest


f = open(data_path + '#1-Yelp_mot.txt', 'w+', encoding='utf-8')
for counter in range(10000):
  f.write(sort_train_Yelp[counter][0] + '\t' + str(counter+1) + '\t' + 
          str(sort_train_Yelp[counter][1]) + '\n')
f.close()

# Commented out IPython magic to ensure Python compatibility.
# # takes time, be patient please
# %%time
# 
# IMDB_train_vec = vector_bin_bag(IMDB_train, 10000, False)
# IMDB_test_vec = vector_bin_bag(IMDB_test, 10000, False)
# IMDB_valid_vec = vector_bin_bag(IMDB_valid, 10000, False)

class_IMDB_train = IMDB_train[:,1].astype(str)
class_IMDB_test = IMDB_test[:,1].astype(str)
class_IMDB_valid = IMDB_valid[:,1].astype(str)

# count the frequency
# sort based on frequency
# descending order
# pick top 10,000 words, ignore the rest

g = open(data_path + '#1-IMDB_mot.txt', 'w+', encoding='utf-8')
for counter in range(10000):
  g.write(sort_train_IMDB[counter][0] + '\t' + str(counter+1) + '\t' + 
          str(sort_train_IMDB[counter][1]) + '\n')
g.close()

# generate dataset Yelp train

with open('#1-Yelp_train.txt', 'w', encoding='utf-8') as f:
  for col in range(Yelp_train.shape[0]):
    mot = Yelp_train[col][0].split(' ')
    temp_Yelp = [(top_train_Yelp.index(verb)+1) if verb in top_train_Yelp else verb for verb in mot]
    for item in temp_Yelp:
      f.write("%s " %item)
    f.write('\t' + class_Yelp_train[col] + '\n')
f.close()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # generate dataset Yelp valid
# 
# with open('#1-Yelp_valid.txt', 'w', encoding='utf-8') as g:
#   for col in range(Yelp_valid.shape[0]):
#     mot = Yelp_valid[col][0].split(' ')
#     temp_Yelp = [(top_train_Yelp.index(verb)+1) if verb in top_train_Yelp else verb for verb in mot]
#     for item in temp_Yelp:
#       if item == temp_Yelp[-1]:
#         g.write("%s" %item)
#       g.write("%s " %item)
#     g.write('\t' + class_Yelp_valid[col] + '\n')
# g.close()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # generate dataset Yelp test
# 
# with open('#1-Yelp_test.txt', 'w', encoding='utf-8') as h:
#   for col in range(Yelp_test.shape[0]):
#     mot = Yelp_test[col][0].split(' ')
#     temp_Yelp = [(top_train_Yelp.index(verb)+1) if verb in top_train_Yelp else verb for verb in mot]
#     for item in temp_Yelp:
#       if item == temp_Yelp[-1]:
#         h.write("%s" %item)
#       h.write("%s " %item)
#       h.write('\t' + class_Yelp_test[col] + '\n')
# h.close()

# Commented out IPython magic to ensure Python compatibility.
# # generate dataset IMDB train
# %%time
# 
# 
# with open('#1-IMDB_train.txt', 'w', encoding='utf-8') as m:
#   for col in range(IMDB_train.shape[0]):
#     mot = IMDB_train[col][0].split(' ')
#     temp_IMDB = [(top_train_IMDB.index(verb)+1) if verb in top_train_IMDB else verb for verb in mot]
#     for item in temp_IMDB:
#       m.write("%s " %item)
#     m.write('\t' + class_IMDB_train[col] + '\n')
# m.close()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # generate dataset IMDB valid
# 
# with open('#1-IMDB_valid.txt', 'w', encoding='utf-8') as n:
#   for col in range(IMDB_valid.shape[0]):
#     mot = IMDB_valid[col][0].split(' ')
#     temp_IMDB = [(top_train_IMDB.index(verb)+1) if verb in top_train_IMDB else verb for verb in mot]
#     for item in temp_IMDB:
#       if item == temp_IMDB[-1]:
#         n.write("%s" %item)
#       n.write("%s " %item)
#       n.write('\t' + class_IMDB_valid[col] + '\n')
# n.close()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # generate dataset IMDB test
# 
# with open('#1-IMDB_test.txt', 'w', encoding='utf-8') as k:
#   for col in range(IMDB_test.shape[0]):
#     mot = IMDB_test[col][0].split(' ')
#     temp_IMDB = [(top_train_IMDB.index(verb)+1) if verb in top_train_IMDB else verb for verb in mot]
#     for item in temp_IMDB:
#       if item == temp_IMDB[-1]:
#         k.write("%s" %item)
#       k.write("%s " %item)
#       k.write('\t' + class_IMDB_test[col] + '\n')
# k.close()

"""># **2.**
For this question, we will focus on the Yelp dataset with binary bag-of-words (BBoW)
representation. We will use the F1-measure as the evaluation metric for the entire
assignment.

>> # **2.(a)** 
As a baseline, report the performance of the random classifier (a classifier which
classifies a review into an uniformly random class) and the majority-class classifier
(a classifier which computes the majority class in the training set and classifies all
test instances as that majority class).
"""

# print('Question #2.a', '\n')
# performance of the random classifier

def uniform_rand_classifier(data_set, data_class, file_name):
  rand = np.empty(shape = (data_class.shape[0]), dtype=int)
  for col in range(data_set.shape[0]):
    if 'Yelp' in file_name:
      rand[col] = np.random.randint(1, 6)
    else:
      rand[col] = np.random.randint(0, 2)
  rand = rand.astype(str)
  f1_random = metrics.f1_score(data_class, rand, average='macro')
  print ('BBoW F1 for random classifier on {} = {}'.format(file_name, f1_random))
  if 'Yelp' in file_name:
    f = open("#2_a-Ranadclass_perf.txt", 'a+')
    f.write('BBoW F1 for random classifier on {} = {} \n'.format(file_name, f1_random))
    f.close()
  else:
    f = open("#4_a-Randclass_perf.txt", 'a+')
    f.write('BBoW F1 for random classifier on {} = {} \n'.format(file_name, f1_random))
    f.close()
  return f1_random, rand

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# f1_Yelp_train, rand_Yelp_train = uniform_rand_classifier(Yelp_train, class_Yelp_train, 'Yelp_train')
# f1_Yelp_test, rand_Yelp_test = uniform_rand_classifier(Yelp_test, class_Yelp_test, 'Yelp_test')
# f1_Yelp_valid, rand_Yelp_valid = uniform_rand_classifier(Yelp_valid, class_Yelp_valid, 'Yelp_valid')

# np.savetxt('#2-a-BBoW_Yelp_train_rand_classifier.csv', rand_Yelp_train, fmt='%s')
# np.savetxt('#2-a-BBoW_Yelp_valid_rand_classifier.csv', rand_Yelp_valid, fmt='%s')
# np.savetxt('#2-a-BBoW_Yelp_test_rand_classifier.csv', rand_Yelp_test, fmt='%s')

# performance of the majority-class classifier
# IMDB is balanced, so majority doesn't make sense

def majority_classifier(data_set, data_class, file_name):
  list_major = {}
  for col in range(Yelp_train.shape[0]):
    if Yelp_train[col][1] in list_major:
      list_major[Yelp_train[col][1]] += 1
    else:
      list_major[data_set[col][1]] = 1
  majority = max(list_major.items(), key = operator.itemgetter(1))[0]
  majority_out = np.full((data_set.shape[0], 1), majority)
  majority_out = np.squeeze(majority_out).astype(str)
  f1_majority = metrics.f1_score(data_class, majority_out, average='micro')
  print('BBoW majority class for {} is {} \n'.format(file_name[0:4], majority))
  print('BBoW F1 for majority classifier on {} = {}'.format(file_name, f1_majority))
  f = open("#2_a-Majority_Perf.txt", 'a+')
  f.write('BBoW majority class for {} is {}. \n F1 for majority classifier on {} = {}.\n'.
          format(file_name[0:4], majority, file_name, f1_majority))
  f.close()
  return f1_majority

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# f1_Yelp_train_majority = majority_classifier(Yelp_train, class_Yelp_train, 'Yelp_train')
# f1_Yelp_test_majority = majority_classifier(Yelp_test, class_Yelp_test, 'Yelp_test')
# f1_Yelp_valid_majority = majority_classifier(Yelp_valid, class_Yelp_valid, 'Yelp_valid')

""">> # **2.(b)**
Now train Naive Bayes, Decision Trees, and Linear SVM for this task. [Note: You
should do a thorough hyper-parameter tuning by using the given validation set.
Also, note that you should use the appropriate naive Bayes classifier for binary
input features (also called Bernoulli naive Bayes).]
"""

# Commented out IPython magic to ensure Python compatibility.
# # print('Question #2.b', '\n')
# #  naive Bayes classifier or Bernoulli naive Bayes
# %%time
# 
# from sklearn.naive_bayes import BernoulliNB
# import numpy as np
# 
# def NB_BBoW(train_vec, ctrain, cvalid, valid_vec, Yelp = True):
#   alphas = [0.0001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 
#             0.007, 0.008, 0.009, 0.01, 0.02, 0.05, 0.1]
#   pred_NB = []
#   f1_NB = []
#   f = open("#2_c-NB_perform.txt", 'a+')
#   g = open("#4_c-NB_perform.txt", 'a+')
#   for i in range(len(alphas)):
#     NB_Class = BernoulliNB(alpha=alphas[i],binarize=None)
#     NB_Class.fit(train_vec, ctrain)
#     test = NB_Class.predict(train_vec)
#     pred_NB.append(NB_Class.predict(valid_vec))
#     F1_train = metrics.f1_score(ctrain, test, average='macro')
#     F1_valid = metrics.f1_score(cvalid, pred_NB[i], average='macro')
#     print("BBoW----alpha:{},  F1_train: {},  F1_valid: {}".
#           format(alphas[i], F1_train, F1_valid))
#     f1_NB.append(metrics.f1_score(cvalid, pred_NB[i], average='macro'))
#     if Yelp:
#       f.write("BBoW----alpha:{},  F1_train: {},  F1_valid: {} \n".
#               format(alphas[i], F1_train, F1_valid))
#     else:
#       g.write("BBoW----alpha:{},  F1_train: {},  F1_valid: {} \n".
#               format(alphas[i], F1_train, F1_valid))
#   alpha_optimum = alphas[f1_NB.index(max(f1_NB))]
#   f.close()
#   g.close()
#   return f1_NB

nb_bbow_Y = NB_BBoW(Yelp_train_vec, class_Yelp_train, class_Yelp_valid, Yelp_valid_vec, True)

# Commented out IPython magic to ensure Python compatibility.
# # decision trees classifier 
# %%time
# 
# from sklearn import tree
# 
# def DecisiontreeBBoW(train_vec, valid_vec, ctrain, cvalid, Yelp = True):
#   depth = [8,9,10,11,12, 15, 20, 25, 30, 40, 50]
#   criterias =  ['gini', 'entropy']
#   pred_tree = []
#   f1_tree = []
#   f = open("#2_c-tree_perform.txt", 'a+')
#   g = open("#4_c-tree_perform.txt", 'a+')
#   for crit in criterias:
#     for i in range(len(depth)):
#       tree_Class = tree.DecisionTreeClassifier(criterion = crit, max_depth=depth[i])
#       tree_Class.fit(train_vec, ctrain)
#       test = tree_Class.predict(train_vec)
#       pred_tree.append(tree_Class.predict(valid_vec))
#       F1_train = metrics.f1_score(ctrain, test, average='macro')
#       F1_valid = metrics.f1_score(cvalid, pred_tree[i], average='macro')
#       print("Depth:{}, criterion:{}, F1_train: {},  F1_valid: {}".
#             format(depth[i], crit, F1_train, F1_valid))
#       f1_tree.append(metrics.f1_score(cvalid, pred_tree[i], average='macro'))
#       if Yelp:
#         f.write("Depth:{}, criterion:{}, F1_train: {},  F1_valid: {} \n".
#                 format(depth[i], crit, F1_train, F1_valid))
#       else:
#         g.write("Depth:{}, criterion:{}, F1_train: {},  F1_valid: {} \n".
#                 format(depth[i], crit, F1_train, F1_valid))        
#   f.close()
#   g.close()

DecisiontreeBBoW(Yelp_train_vec, Yelp_valid_vec, class_Yelp_train, class_Yelp_valid, True)

# Commented out IPython magic to ensure Python compatibility.
# # Linear SVM for Yelp
# %%time
# 
# from warnings import filterwarnings
# from sklearn.svm import LinearSVC
# filterwarnings('ignore')
# 
# 
# def svm_BBoW(train_vec, ctrain, valid_vec, cvalid, Yelp = True):
#   C_svm = [2**-4, 2**-3, 2**-2, 0.5, 1, 1.5, 2, 4,10.0, 2**4, 2**5]
#   pred_svm = []
#   f1_svm = []
#   f = open("#2_c-svm_perform.txt", 'a+')
#   g = open("#4_c-svm_perform.txt", 'a+')
#   for i in range(len(C_svm)):
#     svm_Class = LinearSVC(C = C_svm[i], multi_class="ovr", loss='hinge')
#     svm_Class.fit(train_vec, ctrain)
#     test = svm_Class.predict(train_vec)
#     pred_svm.append(svm_Class.predict(valid_vec))
#     F1_train = metrics.f1_score(ctrain, test, average='macro')
#     F1_valid = metrics.f1_score(cvalid, pred_svm[i], average='macro')
#     print("C = {}, F1_train = {},  F1_valid = {}".
#           format(C_svm[i], F1_train, F1_valid))
#     f1_svm.append(F1_valid)
#     if Yelp:
#       f.write("Depth C = {}, F1_train = {},  F1_valid = {} \n".
#               format(C_svm[i], F1_train, F1_valid))
#     else:
#       g.write("Depth C = {}, F1_train = {},  F1_valid = {} \n".
#               format(C_svm[i], F1_train, F1_valid))
#   f.close()
#   g.close()

svm_BBoW(Yelp_train_vec, class_Yelp_train, Yelp_valid_vec, class_Yelp_valid,  True)

""">> # **2.(c)**
 Report the list of hyper-parameters you considered for each classifier, the range
of the individual hyper-parameters and the best value for these hyper-parameters
chosen based on the validation set performance.
"""

# F1 score for the optimal NB-BBoW hyperparameter 

alphas = [0.0001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.05, 0.1]
alpha_optimum = alphas[nb_bbow_Y.index(max(nb_bbow_Y))]
print('The optimal alpha for the NB classifier is {}'.format(alpha_optimum))
f = open("#2_c-NB_parameters.txt", 'a+')
f.write("The optimal alpha from the set {} w.r.t NB_classifier is {} \n".
        format(alphas, alpha_optimum))
f.close()


def Yelp_optimal_f1(vector, data_class, file_name):
  NB_Class = BernoulliNB(alpha = alpha_optimum)
  NB_Class.fit(Yelp_train_vec, class_Yelp_train)
  predicted_result = (NB_Class.predict(vector))
  f1_result = metrics.f1_score(data_class, predicted_result, average='macro')
  print('NB_BBoW best values for {} with optimal hyperparameters is {}'.
        format(file_name, f1_result))
  f = open("#2_c-NB_parameters.txt", 'a+')
  f.write('{} \n NB_BBoW best values for {} with optimal hyperparameters is {}. \n'.
        format(seperator, file_name, f1_result))

from sklearn.dummy import DummyClassifier as DC
from sklearn.model_selection import ParameterGrid
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn import svm, metrics
from sklearn.metrics import f1_score

seperator = '#'*100
NB_params = ParameterGrid({'alpha':[0.001, 0.002, 0.003, 0.004, 0.005, 
                                    0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.05, 0.1]})
tree_params = ParameterGrid({'criterion':['gini','entropy'],
                             'max_features': [None, "sqrt", "log2"],
                             'max_depth':[8, 9, 10, 15, 20, 30, 50, 100],
                             'min_samples_leaf': [0.2, 0.49],
                             'min_samples_split':[2, 3, 5, 10, 20]})
svm_params = ParameterGrid({'loss':['hinge','squared_hinge'],
                            'C':[0.25, 0.5, 1.5, 2, 5, 10, 100]})

classifiers = [(BernoulliNB, NB_params), (DTC, tree_params), (svm.LinearSVC, svm_params)]

def model_validation(model):
  model.fit(Yelp_train_vec, class_Yelp_train)
  valid = model.predict(Yelp_valid_vec)
  f1_valid = f1_score(class_Yelp_valid, valid, average = 'micro')
  return f1_valid

###########################################################################

def optimize_parameters(classifier, grid, Yelp = True):
  if Yelp:
    f = open("#2_c-All Parameters.txt", 'a+')
    best_f1 = 0.0
    optimal_params = None
    for params in grid:
      f1 = model_validation(classifier(**params))
      f.write("{}-----> F1 = {}\n".format(params, f1))
      print("{}-----> F1 = {}".format(params, f1))
      if f1 > best_f1:
          best_f1 = f1
          optimal_params = params       
    print("Best params: {}".format(optimal_params))
    print("Best F1 Score: {}\n {}".format(best_f1, seperator))
    f.write("Best params: {}\n Best F1 Score = {}\n{}\n".
            format(optimal_params, best_f1, seperator))
    f.close()
  else:
    g = open("#4_c-All Parameters.txt", 'a+')
    best_f1 = 0.0
    optimal_params = None
    for params in grid:
      f1 = model_validation_4(classifier(**params))
      g.write("{} --------> F1 = {}\n".format(params, f1))
      print("{} --------> F1 = {}".format(params, f1))
      if f1 > best_f1:
          best_f1 = f1
          optimal_params = params       
    print("Best params: {}".format(optimal_params))
    print("Best F1 Score: {}\n {} \n".format(best_f1, seperator))
    g.write("Best params: {}\n Best F1 Score = {}\n{}\n".
            format(optimal_params, best_f1, seperator))
    g.close()    
  return classifier(**optimal_params)

###########################################################################

def model_train_valid_test(model):
  model.fit(Yelp_train_vec, class_Yelp_train)
  train = model.predict(Yelp_train_vec)
  valid = model.predict(Yelp_valid_vec)
  test = model.predict(Yelp_test_vec)
  f1_train = f1_score(class_Yelp_train, train, average='micro')
  f1_valid = f1_score(class_Yelp_valid, valid, average='micro')
  f1_test = f1_score(class_Yelp_test, test, average='micro')
  return f1_train, f1_valid, f1_test

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# uni_random = DC(strategy = 'uniform')
# majority_c = DC(strategy = 'most_frequent')
# 
# print('Random Classifier = {:<25}'.format(model_validation(uni_random)))
# print('Majority Classifier = {:<25}'.format(model_validation(majority_c)))
# 
# for param in classifiers:
#     clf = param[0]
#     param_grid = param[1]
#     print(clf)    
#     best_clf = optimize_parameters(clf, param_grid)

""">> # **2.(d)**
Report training, validation, and test F1-measure for all the classifiers (with best
hyper-parameter configuration).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# Yelp_train_F1_NB_optparam = Yelp_optimal_f1(Yelp_train_vec, class_Yelp_train, 'Yelp_train')
# Yelp_test_F1_NB_optparam = Yelp_optimal_f1(Yelp_test_vec, class_Yelp_test, 'Yelp_test')
# Yelp_valid_F1_NB_optparam = Yelp_optimal_f1(Yelp_valid_vec, class_Yelp_valid, 'Yelp_valid')

import re
from sklearn.tree import DecisionTreeClassifier

params = []
with open("#2_c-All Parameters.txt") as f:
  for line in f:
    if 'Best params:' in line:
      params.append(line)
temp1 = repr(params[0])
temp1 = re.sub('Best params: ', '', temp1)
NB_best_param = float(temp1.split()[1][:-4])
with open("#2_d-Best_Results.txt", 'a+') as g:
  g.write('NB_best alpha = {}\n'.format(NB_best_param))
print('NB_best alpha = {}'.format(NB_best_param))
#################################################

temp2 = repr(params[1])
temp2 = re.sub('Best params: ', '', temp2)
criterion = temp2.split()[1][:-1]
criterion = criterion.replace("'",'')
max_dep = [int(s) for s in temp2.split()[3] if s.isdigit()][0]
max_feat = temp2.split()[5][:-1]
if max_feat==str(None):
  max_feat = None
else:
  max_feat = max_feat.replace("'", '')
min_leaf = float(temp2.split()[7][:-1])
mss = int(temp2.split()[9][:-4])
tree_best_params = [criterion, max_dep, max_feat, min_leaf, mss]
with open("#2_d-Best_Results.txt", 'a+') as g:
  g.write('[criterion, max_depth, max_features, min_samples_leaf, min_samples_split] = {}\n'.
          format(tree_best_params))
print('criterion, max_depth, max_features, min_samples_leaf, min_samples_split =', tree_best_params)
##################################################

temp3 = repr(params[2])
temp3 = re.sub('Best params: ', '', temp3)
c = float(temp3.split()[1][:-1])
temp3_loss = temp3.split()[3][:-4]
temp3_loss = temp3_loss.replace("'","" )
svm_best_params = [c, temp3_loss]
with open("#2_d-Best_Results.txt", 'a+') as g:
  g.write('[C, loss] = {}\n {}'.
          format(svm_best_params, seperator))
print('[C, loss] = ', svm_best_params)

print('NB_best_params are {},\n tree_best_params are {},\n svm_best_params are {}.'.
      format(NB_best_param, tree_best_params, svm_best_params))

optimized_bayes = BernoulliNB(alpha = NB_best_param)
optimized_tree = DecisionTreeClassifier(criterion = tree_best_params[0],
                                        max_features = tree_best_params[2],
                                        max_depth = float(tree_best_params[1]),
                                        min_samples_leaf = tree_best_params[3],
                                        min_samples_split = tree_best_params[4], 
                                        random_state = 42)
optimized_svm = svm.LinearSVC(C = svm_best_params[0], loss = svm_best_params[1], random_state = 42)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print('Optimal NB f1 for train, valid, test = {}'.format(model_train_valid_test(optimized_bayes)))
# print('Optimal Tree f1 for train, valid, test = {}'.format(model_train_valid_test(optimized_tree)))
# print('Optimal SVM f1 for train, valid, test = {}'.format(model_train_valid_test(optimized_svm)))
# 
# f = open("#2_d-Best_Results.txt", 'a+')
# f.write('\n NB_best_params are {}, \n tree_best_params are {}, \n svm_best_params are {} \n {}\n'.
#         format(NB_best_param, tree_best_params, svm_best_params, seperator))
# f.write('Optimal NB f1 for the (train, valid, test) = {},\n Optimal Tree f1 for train, valid, test = {}, \n Optimal SVM f1 for train, valid, test = {}\n {}'.
#         format(model_train_valid_test(optimized_bayes), model_train_valid_test(optimized_tree), model_train_valid_test(optimized_svm), seperator))
# f.close()

""">>**2.(e)**
Comment about the performance of different classifiers. Why did a particular
classifier performed better than the others? What was the role of that hyperparameter that fetched you the best results.

> # **3**
Now we will repeat question 2 but with frequency bag-of-words (FBoW) representation.

>> # **3.(a)**

Train Naive Bayes, Decision Trees, and Linear SVM for this task. [Note: You should do a thorough hyper-parameter tuning by using the given validation set. Also, note that you should use the appropriate naive Bayes classifier for real valued input features (also called Gaussian naive Bayes).]
"""

print('Question #3.a', '\n')
# random classifier

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import string
import operator
from sklearn import preprocessing
from sklearn.naive_bayes import BernoulliNB
from sklearn import metrics
from sklearn import tree
from sklearn.svm import LinearSVC


data_path = '/content/'
out = set(string.punctuation)
df_train_3 = pd.read_csv(data_path + 'yelp-train.txt', sep='\t', header=None)
df_test_3 = pd.read_csv(data_path + 'yelp-test.txt', sep='\t', header=None)
df_valid_3 = pd.read_csv(data_path + 'yelp-valid.txt', sep='\t', header=None)

def bin_bag_of_words(data_set):
  counti = data_set[0].count()
  for counter in range(counti):
    senti = data_set[0][counter]
    senti = ''.join(char for char in senti if char not in out)
    senti = senti.lower()
    data_set[0][counter] = senti
  return data_set


def top_myriad(data_set):
  pre_data = {}
  for col in range(data_set.shape[0]):
    sep = data_set[col][0].split()
    for idems in range(len(sep)):
      if sep[idems] in pre_data:
        pre_data[sep[idems]] += 1
      else:
        pre_data[sep[idems]] = 1
  return pre_data

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# FBoW_freq_Yelp_train = bin_bag_of_words (df_train_3)
# FBoW_freq_Yelp_test = bin_bag_of_words (df_test_3)
# FBoW_freq_Yelp_valid = bin_bag_of_words (df_valid_3)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# FBoW_freq_Yelp_train = df_train_3.values
# FBoW_freq_Yelp_test = df_test_3.values
# FBoW_freq_Yelp_valid = df_valid_3.values

top_train_Yelp_FBoW = []
sortit_Yelp_FBoW = top_myriad(FBoW_freq_Yelp_train).items()
sort_train_Yelp_FBoW = sorted(sortit_Yelp_FBoW, key=operator.itemgetter(1), reverse=True)
for i in range(10000):
  top_train_Yelp_FBoW.append(sort_train_Yelp_FBoW[i][0])

# Commented out IPython magic to ensure Python compatibility.
# # frequency bag-of-words for train, validation, test
# %%time
# 
# 
# def frequency_classifier(data_set, file_name, n, top):
#   rand = np.zeros(shape = (data_set.shape[0], n))
#   for col in range(data_set.shape[0]):
#     mot = data_set[col][0].split(' ')
#     for counter in range(n):
#       rand[col][counter] = mot.count(top[counter])
#     if (np.sum(rand[col]) == 0):
#       continue
#     rand[col] = rand[col]/(np.sum(rand[col]))
#   return rand

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# freq_Yelp_train_vec = frequency_classifier(FBoW_freq_Yelp_train, 'FBoW_Yelp_train', 
#                                            10000, top_train_Yelp_FBoW)
# freq_Yelp_test_vec = frequency_classifier(FBoW_freq_Yelp_test, 'FBoW_Yelp_test', 
#                                           10000, top_train_Yelp_FBoW)
# freq_Yelp_valid_vec = frequency_classifier(FBoW_freq_Yelp_valid, 'FBoW_Yelp_valid', 
#                                            10000, top_train_Yelp_FBoW)

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# 
# class_Yelp_train_freq = FBoW_freq_Yelp_train[:,1].astype(str)
# class_Yelp_valid_freq = FBoW_freq_Yelp_valid[:,1].astype(str)
# class_Yelp_test_freq = FBoW_freq_Yelp_test[:,1].astype(str)

# Commented out IPython magic to ensure Python compatibility.
# # random classifier
# %%time
# 
# def FBoW_rand_classifier(data_set, data_class, file_name, n):
#   rand = np.empty(shape=(data_class.shape[0]), dtype=int)
#   for col in range(data_set.shape[0]):
#     rand[col] = np.random.randint(1, 6)
#   rand = rand.astype(str)
#   f1_random = metrics.f1_score(data_class, rand, average='macro')
#   print ('FBoW F1 for random classifier on {} = {}'.format(file_name, f1_random))
#   f = open("#3_b-Parameters.txt", 'a+')
#   f.write('FBoW F1 for random classifier on {} = {} \n'.format(file_name, f1_random))
#   f.close()
#   return f1_random, rand

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rand_Yelp_train_FBoW = FBoW_rand_classifier(FBoW_freq_Yelp_train, class_Yelp_train_freq, 
#                                             'Yelp_train', 10000)
# rand_Yelp_test_FBoW = FBoW_rand_classifier(FBoW_freq_Yelp_test, class_Yelp_test_freq, 
#                                            'Yelp_test', 10000)
# rand_Yelp_valid_FBoW = FBoW_rand_classifier(FBoW_freq_Yelp_valid, class_Yelp_valid_freq, 
#                                             'Yelp_valid', 10000)

# Commented out IPython magic to ensure Python compatibility.
# # majority classifier
# %%time
# 
# 
# def majority_freq_class(data_set, data_class, file_name):
#   list_major = {}
#   for col in range(FBoW_freq_Yelp_train.shape[0]):
#     if FBoW_freq_Yelp_train[col][1] in list_major:
#       list_major[FBoW_freq_Yelp_train[col][1]] += 1
#     else:
#       list_major[data_set[col][1]] = 1
#   majority = max(list_major.items(), key = operator.itemgetter(1))[0]
#   majority_out = np.full((data_set.shape[0], 1), majority)
#   majority_out = np.squeeze(majority_out).astype(str)
#   f1_majority = metrics.f1_score(data_class, majority_out, average='micro')
#   print('FBoW majority class for {} is {} \n'.format(file_name[0:4], majority))
#   print('FBoW F1 for majority classifier on {} = {}'.format(file_name, f1_majority))
#   f = open("#3_b-Parameters.txt", 'a+')
#   f.write('BBoW majority class for {} is {}. \n F1 for majority classifier on {} = {}.\n'.
#           format(file_name[0:4], majority, file_name, f1_majority))
#   f.close()
#   return f1_majority

f1_Y_train_major_FBoW = majority_freq_class(FBoW_freq_Yelp_train, class_Yelp_train_freq, 'Yelp_train')
f1_Y_test_major_FBoW = majority_freq_class(FBoW_freq_Yelp_test, class_Yelp_test_freq, 'Yelp_test')
f1_Y_valid_major_FBoW = majority_freq_class(FBoW_freq_Yelp_valid, class_Yelp_valid_freq, 'Yelp_valid')

# Commented out IPython magic to ensure Python compatibility.
# # NB classifier
# ## Gaussian Bayes multiclass classifier
# 
# %%time
# 
# from sklearn.naive_bayes import GaussianNB
# 
# def NB_FBoW(vec, cfreq, file_name, Yelp = True):
#   f1_NB = []
#   g = open("#5_b-NB_perform.txt", 'a+')
#   f = open("#3_b-Parameters.txt", 'a+')
#   gs_clf = GaussianNB()
#   gs_clf.fit(freq_Yelp_train_vec,class_Yelp_train_freq)
#   y_pred = (gs_clf.predict(vec))
#   f1_NB_FBoW = metrics.f1_score(cfreq, y_pred, average='macro')
#   print('F1 score FBoW for Gaussian NB with optimal hyperparameter on {} = {}'.
#         format(file_name, f1_NB_FBoW))
#   if Yelp:
#     f.write('FBoW NB F1 with optimal params for {} = {}\n'.format(file_name, f1_NB_FBoW))
#     f.close()
#   else:
#     g.write('FBoW NB F1 with optimal params for {} = {}\n'.format(file_name, f1_NB_FBoW))
#     g.close()
#   return f1_NB_FBoW

freq_Yelp_train_f1NB = NB_FBoW(freq_Yelp_train_vec, class_Yelp_train_freq, 'training', True)
freq_Yelp_test_f1NB = NB_FBoW(freq_Yelp_test_vec, class_Yelp_test_freq, 'test set', True)
freq_Yelp_valid_f1NB = NB_FBoW(freq_Yelp_valid_vec, class_Yelp_valid_freq, 'validation', True)

NB_FBoW_opt = max

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # decision trees classifier 
# 
# from sklearn import tree
# 
# 
# def Decisiontree_FBoW(train_vec, valid_vec, ctrain, cvalid, Yelp = True):
#   depth_FBoW = [8,9,10,11,12, 15, 20, 25, 30, 40, 50]
#   criterias =  ['gini', 'entropy']
#   pred_tree = []
#   f1_tree = []
#   f = open("#3_b-tree_perform.txt", 'a+')
#   g = open("#5_b-tree_perform.txt", 'a+')
#   for crit in criterias:
#     for i in range(len(depth_FBoW)):
#       tree_Class = tree.DecisionTreeClassifier(criterion = crit, max_depth=depth_FBoW[i])
#       tree_Class.fit(train_vec, ctrain)
#       test = tree_Class.predict(train_vec)
#       pred_tree.append(tree_Class.predict(valid_vec))
#       F1_train = metrics.f1_score(ctrain, test, average='macro')
#       F1_valid = metrics.f1_score(cvalid, pred_tree[i], average='macro')
#       print("Depth:{:<4}, criterion:{:<10}, F1_train: {:<22},  F1_valid: {:<22}".
#             format(depth_FBoW[i], crit, F1_train, F1_valid))
#       f1_tree.append(metrics.f1_score(cvalid, pred_tree[i], average='macro'))
#       if Yelp:
#         f.write("Depth:{:<4}, criterion:{:<10}, F1_train: {:<22},  F1_valid: {:>22} \n".
#                 format(depth_FBoW[i], crit, F1_train, F1_valid))
#       else:
#         g.write("Depth:{:<4}, criterion:{:<10}, F1_train: {:<22},  F1_valid: {:>22} \n".
#                 format(depth_FBoW[i], crit, F1_train, F1_valid))        
#   f.close()
#   g.close()

Decisiontree_FBoW(freq_Yelp_train_vec, freq_Yelp_valid_vec, 
                 class_Yelp_train_freq, class_Yelp_valid_freq, True)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Linear SVM for Yelp
# 
# from warnings import filterwarnings
# from sklearn.svm import LinearSVC
# filterwarnings('ignore')
# 
# 
# def svm_FBoW(train_vec, ctrain, valid_vec, cvalid, Yelp = True):
#   C_svm = [2**-4, 2**-3, 2**-2, 0.5, 1, 1.5, 2, 4,10.0, 2**4, 2**5]
#   pred_svm = []
#   f1_svm = []
#   f = open("#3_b-svm_perform.txt", 'a+')
#   g = open("#5_b-svm_perform.txt", 'a+')
#   for i in range(len(C_svm)):
#     svm_Class = LinearSVC(C = C_svm[i], multi_class="ovr", loss='hinge')
#     svm_Class.fit(train_vec, ctrain)
#     test = svm_Class.predict(train_vec)
#     pred_svm.append(svm_Class.predict(valid_vec))
#     F1_train = metrics.f1_score(ctrain, test, average='macro')
#     F1_valid = metrics.f1_score(cvalid, pred_svm[i], average='macro')
#     print("C = {:<8}, F1_train = {:<24},  F1_valid = {:<24}".
#           format(C_svm[i], F1_train, F1_valid))
#     f1_svm.append(F1_valid)
#     if Yelp:
#       f.write("C = {:<8}, F1_train = {:<24},  F1_valid = {:>24}\n".
#               format(C_svm[i], F1_train, F1_valid))
#     else:
#       g.write("C = {:<8}, F1_train = {:<24},  F1_valid = {:>24}\n".
#               format(C_svm[i], F1_train, F1_valid))
#   f.close()
#   g.close()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Linear SVM for Yelp
# 
# svm_FBoW(freq_Yelp_train_vec, class_Yelp_train_freq, 
#          freq_Yelp_valid_vec, class_Yelp_valid_freq, True)

""">> # **3.(b)**
 Report the list of hyper-parameters you considered for each classifier, the range
of the individual hyper-parameters and the best value for these hyper-parameters
chosen based on the validation set performance.
"""

from sklearn.dummy import DummyClassifier as DC
from sklearn.model_selection import ParameterGrid
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn import svm, metrics
from sklearn.metrics import f1_score

seperator = '#'*100
NB_params = ParameterGrid({'alpha':[0.001, 0.002, 0.003, 0.004, 0.005, 
                                    0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.05, 0.1]})
tree_params = ParameterGrid({'criterion':['gini','entropy'],
                             'max_features': [None, "sqrt", "log2"],
                             'max_depth':[8, 9, 10, 15, 20, 30, 50, 100],
                             'min_samples_leaf': [0.2, 0.49],
                             'min_samples_split':[2, 3, 5, 10, 20]})
svm_params = ParameterGrid({'loss':['hinge','squared_hinge'],
                            'C':[0.25, 0.5, 1.5, 2, 5, 10, 100]})

classifiers = [(BernoulliNB, NB_params), (DTC, tree_params), (svm.LinearSVC, svm_params)]

def model_validation_FBoW_3(model):
  model.fit(freq_Yelp_train_vec, class_Yelp_train_freq)
  valid = model.predict(freq_Yelp_valid_vec)
  f1_valid = f1_score(class_Yelp_valid_freq, valid, average = 'micro')
  return f1_valid

###########################################################################

def optimize_parameters_FBoW(classifier, grid, Yelp = True):
  if Yelp:
    f = open("#3_b-All Parameters.txt", 'a+')
    best_f1 = 0.0
    optimal_params = None
    for params in grid:
      f1 = model_validation_FBoW_3(classifier(**params))
      f.write("{}-----> F1 = {}\n".format(params, f1))
      print("{}-----> F1 = {}".format(params, f1))
      if f1 > best_f1:
          best_f1 = f1
          optimal_params = params       
    print("Best params: {}".format(optimal_params))
    print("Best F1 Score: {}\n {}".format(best_f1, seperator))
    f.write("Best params: {}\n Best F1 Score = {}\n{}\n".format(optimal_params, best_f1, seperator))
    f.close()
  else:
    g = open("#5_b-All Parameters.txt", 'a+')
    best_f1 = 0.0
    optimal_params = None
    for params in grid:
      f1 = model_validation_FBoW_5(classifier(**params))
      g.write("{} --------> F1 = {}\n".format(params, f1))
      print("{} --------> F1 = {}".format(params, f1))
      if f1 > best_f1:
          best_f1 = f1
          optimal_params = params       
    print("Best params: {}".format(optimal_params))
    print("Best F1 Score: {}\n {} \n".format(best_f1, seperator))
    g.write("Best params: {}\n Best F1 Score = {}\n{}\n".format(optimal_params, best_f1, seperator))
    g.close()    
  return classifier(**optimal_params)

###########################################################################

def model_train_valid_test_FBoW_3(model):
  model.fit(freq_Yelp_train_vec, class_Yelp_train_freq)
  train = model.predict(freq_Yelp_train_vec)
  valid = model.predict(freq_Yelp_valid_vec)
  test = model.predict(freq_Yelp_test_vec)
  f1_train = f1_score(class_Yelp_train_freq, train, average='micro')
  f1_valid = f1_score(class_Yelp_valid_freq, valid, average='micro')
  f1_test = f1_score(class_Yelp_test_freq, test, average='micro')
  return f1_train, f1_valid, f1_test

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# uni_random = DC(strategy = 'uniform')
# majority_c = DC(strategy = 'most_frequent')
# 
# print('Random Classifier = {}'.format(model_validation_FBoW_3(uni_random)))
# print('Majority Classifier = {}'.format(model_validation_FBoW_3(majority_c)))
# 
# for param in classifiers:
#     clf = param[0]
#     param_grid = param[1]
#     print(clf)    
#     best_clf = optimize_parameters_FBoW(clf, param_grid)

""">> # **3.(c)**
 Report training, validation, and test F1-measure for all the classifiers (with best
hyper-parameter configuration).
"""

import re
from sklearn.tree import DecisionTreeClassifier

params_3 = []
with open("#3_b-All Parameters.txt") as f:
  for line in f:
    if 'Best params:' in line:
      params_3.append(line)
temp1_3 = repr(params_3[0])
temp1_3 = re.sub('Best params: ', '', temp1_3)
NB_best_param_3 = float(temp1_3.split()[1][:-4])
with open("#3_c-Best_Results.txt", 'a+') as g:
  g.write('NB_best alpha = {}\n'.format(NB_best_param_3))
print('NB_best alpha = {}'.format(NB_best_param_3))
#################################################

temp2_3 = repr(params_3[1])
temp2_3 = re.sub('Best params: ', '', temp2_3)
criterion_3 = temp2_3.split()[1][:-1]
criterion_3 = criterion_3.replace("'",'')
max_dep_3 = [int(s) for s in temp2_3.split()[3] if s.isdigit()][0]
max_feat_3 = temp2_3.split()[5][:-1]
if max_feat_3 == str(None):
  max_feat_3 = None
else:
  max_feat_3 = max_feat_3.replace("'", '')
min_leaf_3 = float(temp2.split()[7][:-1])
mss_3 = int(temp2_3.split()[9][:-4])
tree_best_params_3 = [criterion_3, max_dep_3, max_feat_3, min_leaf_3, mss_3]
with open("#2_d-Best_Results.txt", 'a+') as g:
  g.write('[criterion, max_depth, max_features, min_samples_leaf, min_samples_split] = {}\n'.
          format(tree_best_params_3))
print('criterion, max_depth, max_features, min_samples_leaf, min_samples_split =', tree_best_params)
##################################################

temp3_3 = repr(params_3[2])
temp3_3 = re.sub('Best params: ', '', temp3_3)
c_3 = float(temp3_3.split()[1][:-1])
temp3_loss_3 = temp3_3.split()[3][:-4]
temp3_loss_3 = temp3_loss_3.replace("'","" )
svm_best_params_3 = [c_3, temp3_loss_3]
with open("#2_d-Best_Results.txt", 'a+') as g:
  g.write('[C, loss] = {}\n {}'.
          format(svm_best_params_3, seperator))
print('[C, loss] = ', svm_best_params_3)

print('NB_best_params are {},\n tree_best_params are {},\n svm_best_params are {}.'.
      format(NB_best_param_3, tree_best_params_3, svm_best_params_3))

optimized_bayes_3 = BernoulliNB(alpha = NB_best_param_3)
optimized_tree_3 = DecisionTreeClassifier(criterion = tree_best_params_3[0],
                                        max_features = tree_best_params_3[2],
                                        max_depth = float(tree_best_params_3[1]),
                                        min_samples_leaf = tree_best_params_3[3],
                                        min_samples_split = tree_best_params_3[4], 
                                        random_state = 42)
optimized_svm_3 = svm.LinearSVC(C = svm_best_params_3[0], loss = svm_best_params_3[1], random_state = 42)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print('Optimal NB f1 for train, valid, test = {}'.
#       format(model_train_valid_test_FBoW_3(optimized_bayes_3)))
# print('Optimal Tree f1 for train, valid, test = {}'.
#       format(model_train_valid_test_FBoW_3(optimized_tree_3)))
# print('Optimal SVM f1 for train, valid, test = {}'.
#       format(model_train_valid_test_FBoW_3(optimized_svm_3)))
# 
# f = open("#3_c-Best_Results.txt", 'a+')
# f.write('\n NB_best_params are {}, \n tree_best_params are {}, \n svm_best_params are {} \n {}\n'.
#         format(NB_best_param_3, tree_best_params_3, svm_best_params_3, seperator))
# f.write('Optimal NB f1 for the (train, valid, test) = {},\n Optimal Tree f1 for train, valid, test = {}, \n Optimal SVM f1 for train, valid, test = {}\n {}'.
#         format(model_train_valid_test_FBoW_3(optimized_bayes_3), 
#                model_train_valid_test_FBoW_3(optimized_tree_3), 
#                model_train_valid_test_FBoW_3(optimized_svm_3), 
#                seperator))
# f.close()

""">> # **3.(d)**
Comment about the performance of different classifiers. Why did a particular
classifier performed better than the others? What was the role of that hyperparameter that fetched you the best results.

>> # **3.(e)**
Compare the performance with the binary bag-of-words based classifiers. Why the
difference in performance? Give a brief explanation comparing BBoW Naive Bayes
and FBoW Naive Bayes and similarly for Decision Trees and Linear SVM.

>> # **3.(f)**
Which representation is better? Why?

> # **4**
 Now we will repeat questions 2 and 3 but with IMDB dataset. For this question we will
use IMDB dataset with BBoW representation.

>> # **4.(a)**
As a baseline, report the performance of the random classifier (a classifier which
classifies a review into an uniformly random class). Note that the IMDB dataset
is a balanced dataset. Hence majority-class classifier doesnâ€™t make sense for this
dataset, and you can omit it for this question.
"""

# was
f1_IMDB_train, rand_IMDB_train = uniform_rand_classifier(IMDB_train, class_IMDB_train, 'IMDB_train')
f1_IMDB_test, rand_IMDB_test = uniform_rand_classifier(IMDB_test, class_IMDB_test, 'IMDB_test')
f1_IMDB_valid, rand_IMDB_valid = uniform_rand_classifier(IMDB_valid, class_IMDB_valid, 'IMDB_valid')

# np.savetxt('#4-a-BBoW_IMDB_train_rand_classifier.csv', rand_IMDB_train, fmt='%s')
# np.savetxt('#4-a-BBoW_IMDB_valid_rand_classifier.csv', rand_IMDB_valid, fmt='%s')
# np.savetxt('#4-a-BBoW_IMDB_test_rand_classifier.csv', rand_IMDB_test, fmt='%s')

""">> # **4.(b)**
 Now train Naive Bayes, Decision Trees, and Linear SVM for this task. [Note: You
should do a thorough hyper-parameter tuning by using the given validation set.
Also, note that you should use the appropriate naive Bayes classifier for binary
input features (also called Bernoulli naive Bayes).]
"""

# Commented out IPython magic to ensure Python compatibility.
# # print('Question #4.b', '\n')
# #  naive Bayes classifier or Bernoulli naive Bayes IMDB
# %%time
# 
# nb_bbow_IMDB = NB_BBoW(IMDB_train_vec, class_IMDB_train, class_IMDB_valid, IMDB_valid_vec, False)

# Commented out IPython magic to ensure Python compatibility.
# # decision trees classifier 
# %%time
# 
# svm_BBoW(IMDB_train_vec, class_IMDB_train, IMDB_valid_vec, class_IMDB_valid,  False)

""">> # **4.(c)**
Report the list of hyper-parameters you considered for each classifier, the range
of the individual hyper-parameters and the best value for these hyper-parameters
chosen based on the validation set performance.
"""

# F1 score for the optimal NB-BBoW hyperparameter 

alphas = [0.0001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 
          0.008, 0.009, 0.01, 0.02, 0.05, 0.1]
alpha_optimum = alphas[nb_bbow_IMDB.index(max(nb_bbow_IMDB))]
print('The optimal alpha for the NB classifier for IMDB is {}'.format(alpha_optimum))
f = open("#4_c-NB_parameters.txt", 'a+')
f.write("The optimal alpha from the set {} for the IMDB dataset w.r.t NB_classifier is {} \n".
        format(alphas, alpha_optimum))
f.close()


def IMDB_optimal_f1(vector, data_class, file_name):
  NB_Class = BernoulliNB(alpha = alpha_optimum)
  NB_Class.fit(IMDB_train_vec, class_IMDB_train)
  predicted_result = (NB_Class.predict(vector))
  f1_result = metrics.f1_score(data_class, predicted_result, average='macro')
  print('NB_BBoW best values for the IMDB {} with optimal hyperparameters is {}'.
        format(file_name, f1_result))
  f = open("#4_d-NB_performance.txt", 'a+')
  f.write('{} \n NB_BBoW best values for the {} of IMDB dataset with optimal hyperparameters is {}. \n'.
        format(seperator, file_name, f1_result))

from sklearn.dummy import DummyClassifier as DC
from sklearn.model_selection import ParameterGrid
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn import svm, metrics
from sklearn.metrics import f1_score

seperator = '#'*100
NB_params = ParameterGrid({'alpha':[0.001, 0.002, 0.003, 0.004, 0.005, 
                                    0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.05, 0.1]})
tree_params = ParameterGrid({'criterion':['gini','entropy'],
                             'max_features': [None, "sqrt", "log2"],
                             'max_depth':[8, 9, 10, 15, 20, 30, 50, 100],
                             'min_samples_leaf': [0.2, 0.49],
                             'min_samples_split':[2, 3, 5, 10, 20]})
svm_params = ParameterGrid({'loss':['hinge','squared_hinge'],
                            'C':[0.25, 0.5, 1.5, 2, 5, 10, 100]})

classifiers = [(BernoulliNB, NB_params), (DTC, tree_params), (svm.LinearSVC, svm_params)]

def model_validation_4(model):
  model.fit(IMDB_train_vec, class_IMDB_train)
  valid = model.predict(IMDB_valid_vec)
  f1_valid = f1_score(class_IMDB_valid, valid, average = 'micro')
  return f1_valid

###########################################################################

def model_train_valid_test_4(model):
  model.fit(IMDB_train_vec, class_IMDB_train)
  train = model.predict(IMDB_train_vec)
  valid = model.predict(IMDB_valid_vec)
  test = model.predict(IMDB_test_vec)
  f1_train = f1_score(class_IMDB_train, train, average='micro')
  f1_valid = f1_score(class_IMDB_valid, valid, average='micro')
  f1_test = f1_score(class_IMDB_test, test, average='micro')
  return f1_train, f1_valid, f1_test

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# uni_random = DC(strategy = 'uniform')
# majority_c = DC(strategy = 'most_frequent')
# 
# print('Random Classifier = {}'.format(model_validation_4(uni_random)))
# # print('Majority Classifier = {:<25}'.format(model_validation_4(majority_c)))
# 
# for param in classifiers:
#     clf = param[0]
#     param_grid = param[1]
#     print(clf)    
#     best_clf = optimize_parameters(clf, param_grid, False)

""">> # **4.(d)**
 Report training, validation, and test F1-measure for all the classifiers (with best
hyper-parameter configuration).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# IMDB_train_F1_NB_optparam = IMDB_optimal_f1(IMDB_train_vec, class_IMDB_train, 'IMDB_train')
# IMDB_test_F1_NB_optparam = IMDB_optimal_f1(IMDB_test_vec, class_IMDB_test, 'IMDB_test')
# IMDB_valid_F1_NB_optparam = IMDB_optimal_f1(IMDB_valid_vec, class_IMDB_valid, 'IMDB_valid')

import re
from sklearn.tree import DecisionTreeClassifier

params_4 = []
with open("#4_c-All Parameters.txt") as f:
  for line in f:
    if 'Best params:' in line:
      params_4.append(line)
temp1_4 = repr(params_4[0])
temp1_4 = re.sub('Best params: ', '', temp1_4)
NB_best_param_4 = float(temp1_4.split()[1][:-4])
print('NB_best alpha IMDB = ',NB_best_param_4)
# #################################################

temp2_4 = repr(params_4[1])
temp2_4 = re.sub('Best params: ', '', temp2_4)
criterion_4 = temp2_4.split()[1][:-1]
criterion_4 = criterion_4.replace("'",'')
max_dep_4 = [int(s) for s in temp2_4.split()[3] if s.isdigit()][0]
max_feat_4 = temp2_4.split()[5][:-1]
if max_feat_4==str(None):
  max_feat_4 = None
else:
  max_feat_4 = max_feat_4.replace("'", '')
min_leaf_4 = float(temp2_4.split()[7][:-1])
mss_4 = int(temp2_4.split()[9][:-4])
tree_best_params_4 = [criterion_4, max_dep_4, max_feat_4, min_leaf_4, mss_4]
print('criterion, max_depth, max_features, min_samples_leaf, min_samples_split =', 
      tree_best_params_4)
##################################################

temp3_4 = repr(params_4[2])
temp3_4 = re.sub('Best params: ', '', temp3_4)
print(temp3_4)
c_4 = float(temp3_4.split()[1][:-1])
temp3_loss_4 = temp3_4.split()[3][:-4]
temp3_loss_4 = temp3_loss_4.replace("'","" )
svm_best_params_4 = [c_4, temp3_loss_4]
print(svm_best_params_4)

print('NB_best_params are {},\n tree_best_params are {},\n svm_best_params are {}.'.
      format(NB_best_param_4, tree_best_params_4, svm_best_params_4))

optimized_bayes_4 = BernoulliNB(alpha = NB_best_param_4)
optimized_tree_4 = DecisionTreeClassifier(criterion = tree_best_params_4[0],
                                        max_features = tree_best_params_4[2],
                                        max_depth = float(tree_best_params_4[1]),
                                        min_samples_split = tree_best_params_4[4], 
                                        random_state = 42)
optimized_svm_4 = svm.LinearSVC(C = svm_best_params_4[0], loss = svm_best_params_4[1], random_state = 42)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print('Optimal NB f1 for train, valid, test = {}'.format(model_train_valid_test_4(optimized_bayes_4)))
# print('Optimal Tree f1 for train, valid, test = {}'.format(model_train_valid_test_4(optimized_tree_4)))
# print('Optimal SVM f1 for train, valid, test = {}'.format(model_train_valid_test_4(optimized_svm_4)))
# 
# f = open("#4_d-Best_Results.txt", 'a+')
# f.write('\n NB_best_params are {}, \n tree_best_params are {}, \n svm_best_params are {} \n {}\n'.
#         format(NB_best_param_4, tree_best_params_4, svm_best_params_4, seperator))
# f.write('Optimal NB f1 for (train, valid, test) = {},\n Optimal Tree f1 for train, valid, test = {}, \n Optimal SVM f1 for train, valid, test = {}\n {}'.
#         format(model_train_valid_test_4(optimized_bayes_4), model_train_valid_test_4(optimized_tree_4), 
#                model_train_valid_test_4(optimized_svm_4), seperator))
# f.close()

""">> # **4.(e)**
Comment about the performance of different classifiers. Why did a particular
classifier performed better than the others? What was the role of that hyperparameter that fetched you the best results.

># **5**
Now we will consider IMDB dataset with frequency bag-of-words (FBoW) representation.

>> # **5.(a)**
Train Naive Bayes, Decision Trees, and Linear SVM for this task. [Note: You
should do a thorough hyper-parameter tuning by using the given validation set.
Also, note that you should use the appropriate naive Bayes classifier for real valued
features (also called Gaussian naive Bayes).]
"""

print('Question #5', '\n')
# random classifier

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import string
import operator
from sklearn import preprocessing
from sklearn.naive_bayes import BernoulliNB
from sklearn import metrics
from sklearn import tree
from sklearn.svm import LinearSVC


data_path = '/content/'
out = set(string.punctuation)
df_train_5 = pd.read_csv(data_path + 'IMDB-train.txt', sep='\t', header=None)
df_test_5 = pd.read_csv(data_path + 'IMDB-test.txt', sep='\t', header=None)
df_valid_5 = pd.read_csv(data_path + 'IMDB-valid.txt', sep='\t', header=None)

def bin_bag_of_words(data_set):
  counti = data_set[0].count()
  for counter in range(counti):
    senti = data_set[0][counter]
    senti = ''.join(char for char in senti if char not in out)
    senti = senti.lower()
    data_set[0][counter] = senti
  return data_set


def top_myriad(data_set):
  pre_data = {}
  for col in range(data_set.shape[0]):
    sep = data_set[col][0].split()
    for idems in range(len(sep)):
      if sep[idems] in pre_data:
        pre_data[sep[idems]] += 1
      else:
        pre_data[sep[idems]] = 1
  return pre_data

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# FBoW_freq_IMDB_train = bin_bag_of_words(df_train_5)
# FBoW_freq_IMDB_test = bin_bag_of_words(df_test_5)
# FBoW_freq_IMDB_valid = bin_bag_of_words(df_valid_5)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# FBoW_freq_IMDB_train = df_train_5.values
# FBoW_freq_IMDB_test = df_test_5.values
# FBoW_freq_IMDB_valid = df_valid_5.values

top_train_IMDB_FBoW = []
sortit_IMDB_FBoW = top_myriad(FBoW_freq_IMDB_train).items()
sort_train_IMDB_FBoW = sorted(sortit_IMDB_FBoW, key=operator.itemgetter(1), reverse=True)
for i in range(10000):
  top_train_IMDB_FBoW.append(sort_train_IMDB_FBoW[i][0])

# Commented out IPython magic to ensure Python compatibility.
# # frequency bag-of-words for train, validation, test
# %%time
# 
# 
# def frequency_classifier(data_set, file_name, n, top):
#   rand = np.zeros(shape = (data_set.shape[0], n))
#   for col in range(data_set.shape[0]):
#     mot = data_set[col][0].split(' ')
#     for counter in range(n):
#       rand[col][counter] = mot.count(top[counter])
#     if (np.sum(rand[col]) == 0):
#       continue
#     rand[col] = rand[col]/(np.sum(rand[col]))
#   return rand

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 30 min
# 
# freq_IMDB_train_vec = frequency_classifier(FBoW_freq_IMDB_train, 'FBoW_IMDB_train', 
#                                            10000, top_train_IMDB_FBoW)
# freq_IMDB_test_vec = frequency_classifier(FBoW_freq_IMDB_test, 'FBoW_IMDB_test', 
#                                           10000, top_train_IMDB_FBoW)
# freq_IMDB_valid_vec = frequency_classifier(FBoW_freq_IMDB_valid, 'FBoW_IMDB_valid', 
#                                            10000, top_train_IMDB_FBoW)

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# 
# class_IMDB_train_freq = FBoW_freq_IMDB_train[:,1].astype(str)
# class_IMDB_valid_freq = FBoW_freq_IMDB_valid[:,1].astype(str)
# class_IMDB_test_freq = FBoW_freq_IMDB_test[:,1].astype(str)

# Commented out IPython magic to ensure Python compatibility.
# # random classifier
# %%time
# 
# def FBoW_rand_class_IMDB(data_set, data_class, file_name, n):
#   rand = np.empty(shape=(data_class.shape[0]), dtype=int)
#   for col in range(data_set.shape[0]):
#     rand[col] = np.random.randint(0, 2)
#   rand = rand.astype(str)
#   f1_random = metrics.f1_score(data_class, rand, average='macro')
#   print ('FBoW F1 for random classifier on {} = {}'.format(file_name, f1_random))
#   f = open("#5_b-Parameters.txt", 'a+')
#   f.write('FBoW F1 for random classifier on {} = {} \n'.format(file_name, f1_random))
#   f.close()
#   return f1_random, rand

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rand_IMDB_train_FBoW = FBoW_rand_class_IMDB(FBoW_freq_IMDB_train, class_IMDB_train_freq, 
#                                             'IMDB_train', 10000)
# rand_IMDB_test_FBoW = FBoW_rand_class_IMDB(FBoW_freq_IMDB_test, class_IMDB_test_freq, 
#                                            'IMDB_test', 10000)
# rand_IMDB_valid_FBoW = FBoW_rand_class_IMDB(FBoW_freq_IMDB_valid, class_IMDB_valid_freq, 
#                                             'IMDB_valid', 10000)

# Commented out IPython magic to ensure Python compatibility.
# # decision trees classifier 
# %%time
# 
# Decisiontree_FBoW(freq_IMDB_train_vec, freq_IMDB_valid_vec, 
#                  class_IMDB_train_freq, class_IMDB_valid_freq, False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Linear SVM for IMDB
# 
# svm_FBoW(freq_IMDB_train_vec, class_IMDB_train_freq, 
#          freq_IMDB_valid_vec, class_IMDB_valid_freq, False)

""">> # **5.(b)**
Report the list of hyper-parameters you considered for each classifier, the range
of the individual hyper-parameters and the best value for these hyper-parameters
chosen based on the validation set performance.
"""

def model_validation_FBoW_5(model):
  model.fit(freq_IMDB_train_vec, class_IMDB_train_freq)
  valid = model.predict(freq_IMDB_valid_vec)
  f1_valid = f1_score(class_IMDB_valid_freq, valid, average = 'micro')
  return f1_valid


def model_train_valid_test_FBoW_5(model):
  model.fit(freq_IMDB_train_vec, class_IMDB_train_freq)
  train = model.predict(freq_IMDB_train_vec)
  valid = model.predict(freq_IMDB_valid_vec)
  test = model.predict(freq_IMDB_test_vec)
  f1_train = f1_score(class_IMDB_train_freq, train, average='micro')
  f1_valid = f1_score(class_IMDB_valid_freq, valid, average='micro')
  f1_test = f1_score(class_IMDB_test_freq, test, average='micro')
  return f1_train, f1_valid, f1_test

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.dummy import DummyClassifier as DC
# from sklearn.metrics import f1_score
# 
# uni_random_5 = DC(strategy = 'uniform')
# majority_c_5 = DC(strategy = 'most_frequent')
# 
# print('Random Classifier = {}'.format(model_validation_FBoW_5(uni_random_5)))
# # print('Majority Classifier = {}'.format(model_validation_FBoW_5(majority_c_5)))
# 
# for param in classifiers:
#     clf_5 = param[0]
#     param_grid_5 = param[1]
#     print(clf_5)    
#     best_clf_5 = optimize_parameters_FBoW(clf_5, param_grid_5, False)

""">> # **5.(c)**
Report training, validation, and test F1-measure for all the classifiers (with best
hyper-parameter configuration).
"""

import re
from sklearn.tree import DecisionTreeClassifier

params_5 = []
with open("#5_b-All Parameters.txt") as f:
  for line in f:
    if 'Best params:' in line:
      params_5.append(line)
temp1_5 = repr(params_5[0])
temp1_5 = re.sub('Best params: ', '', temp1_5)
NB_best_param_5 = float(temp1_5.split()[1][:-4])
with open("#5_c-Best_Results.txt", 'a+') as g:
  g.write('NB_best alpha = {}\n'.format(NB_best_param_5))
print('NB_best alpha = {}'.format(NB_best_param_5))
#################################################

temp2_5 = repr(params_5[1])
temp2_5 = re.sub('Best params: ', '', temp2_5)
criterion_5 = temp2_5.split()[1][:-1]
criterion_5 = criterion_5.replace("'",'')
max_dep_5 = [int(s) for s in temp2_5.split()[3] if s.isdigit()][0]
max_feat_5 = temp2_5.split()[5][:-1]
if max_feat_5 == str(None):
  max_feat_5 = None
else:
  max_feat_5 = max_feat_5.replace("'", '')
min_leaf_5 = float(temp2_5.split()[7][:-1])
mss_5 = int(temp2_5.split()[9][:-4])
tree_best_params_5 = [criterion_5, max_dep_5, max_feat_5, min_leaf_5, mss_5]
with open("#5_c-Best_Results.txt", 'a+') as g:
  g.write('[criterion, max_depth, max_features, min_samples_leaf, min_samples_split] = {}\n'.
          format(tree_best_params_5))
print('criterion, max_depth, max_features, min_samples_leaf, min_samples_split =', tree_best_params_5)
##################################################

temp3_5 = repr(params_5[2])
temp3_5 = re.sub('Best params: ', '', temp3_5)
c_5 = float(temp3_5.split()[1][:-1])
temp3_loss_5 = temp3_5.split()[3][:-4]
temp3_loss_5 = temp3_loss_5.replace("'","" )
svm_best_params_5 = [c_5, temp3_loss_5]
with open("#5_c-Best_Results.txt", 'a+') as g:
  g.write('[C, loss] = {}\n {}'.
          format(svm_best_params_5, seperator))
print('[C, loss] = ', svm_best_params_5)

print('NB_best_params are {},\n tree_best_params are {},\n svm_best_params are {}.'.
      format(NB_best_param_5, tree_best_params_5, svm_best_params_5))

optimized_bayes_5 = BernoulliNB(alpha = NB_best_param_5)
optimized_tree_5 = DecisionTreeClassifier(criterion = tree_best_params_5[0],
                                        max_features = tree_best_params_5[2],
                                        max_depth = float(tree_best_params_5[1]),
                                        min_samples_leaf = tree_best_params_5[3],
                                        min_samples_split = tree_best_params_5[4], 
                                        random_state = 42)
optimized_svm_5 = svm.LinearSVC(C = svm_best_params_5[0], loss = svm_best_params_5[1], random_state = 42)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print('Optimal NB f1 for train, valid, test = {}'.
#       format(model_train_valid_test_FBoW_5(optimized_bayes_5)))
# print('Optimal Tree f1 for train, valid, test = {}'.
#       format(model_train_valid_test_FBoW_5(optimized_tree_5)))
# print('Optimal SVM f1 for train, valid, test = {}'.
#       format(model_train_valid_test_FBoW_5(optimized_svm_5)))
# 
# f = open("#5_c-Best_Results.txt", 'a+')
# f.write('\n NB_best_params are {}, \n tree_best_params are {}, \n svm_best_params are {} \n {}\n'.
#         format(NB_best_param_5, tree_best_params_5, svm_best_params_5, seperator))
# f.write('Optimal NB f1 for the (train, valid, test) = {},\n Optimal Tree f1 for train, valid, test = {}, \n Optimal SVM f1 for train, valid, test = {}\n {}'.
#         format(model_train_valid_test_FBoW_5(optimized_bayes_5), 
#                model_train_valid_test_FBoW_5(optimized_tree_5), 
#                model_train_valid_test_FBoW_5(optimized_svm_5), 
#                seperator))
# f.close()

""">> # **5.(d)**
Comment about the performance of different classifiers. Why did a particular
classifier performed better than the others? What was the role of that hyperparameter that fetched you the best results.

>> # **5.(e)**
Compare the performance with the binary bag-of-words based classifiers. Why the
difference in performance? Give a brief explanation comparing BBoW Naive Bayes
and FBoW Naive Bayes and similarly for Decision Trees and Linear SVM.

>> # **5.(f)**
Which representation is better ? Why ?

>> # **5.(g)**
What did you observe about the relative performance of the classifiers when the
dataset is changed? What aspects of the dataset aided or hampered the performance
of that classifier.
"""